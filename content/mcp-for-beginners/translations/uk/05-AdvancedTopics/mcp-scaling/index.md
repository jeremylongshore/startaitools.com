<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7b4d8d17fc1f501468cce40c3651aed1",
  "translation_date": "2025-07-17T13:02:59+00:00",
  "source_file": "05-AdvancedTopics/mcp-scaling/README.md",
  "language_code": "uk"
}
-->
# Масштабованість і високопродуктивний MCP

Для корпоративних розгортань реалізації MCP часто повинні обробляти великі обсяги запитів з мінімальною затримкою.

## Вступ

У цьому уроці ми розглянемо стратегії масштабування серверів MCP для ефективної обробки великих навантажень. Ми охопимо горизонтальне та вертикальне масштабування, оптимізацію ресурсів і розподілені архітектури.

## Цілі навчання

Після завершення цього уроку ви зможете:

- Реалізувати горизонтальне масштабування за допомогою балансування навантаження та розподіленого кешування.
- Оптимізувати сервери MCP для вертикального масштабування та управління ресурсами.
- Проєктувати розподілені архітектури MCP для високої доступності та відмовостійкості.
- Використовувати передові інструменти та методи для моніторингу продуктивності та оптимізації.
- Застосовувати найкращі практики масштабування серверів MCP у виробничих середовищах.

## Стратегії масштабування

Існує кілька способів ефективно масштабувати сервери MCP:

- **Горизонтальне масштабування**: Розгортання кількох екземплярів серверів MCP за балансувальником навантаження для рівномірного розподілу вхідних запитів.
- **Вертикальне масштабування**: Оптимізація одного екземпляра сервера MCP для обробки більшої кількості запитів шляхом збільшення ресурсів (CPU, пам’ять) та тонкого налаштування конфігурацій.
- **Оптимізація ресурсів**: Використання ефективних алгоритмів, кешування та асинхронної обробки для зменшення споживання ресурсів і покращення часу відгуку.
- **Розподілена архітектура**: Реалізація розподіленої системи, де кілька вузлів MCP працюють разом, розподіляючи навантаження та забезпечуючи резервування.

## Горизонтальне масштабування

Горизонтальне масштабування передбачає розгортання кількох екземплярів серверів MCP і використання балансувальника навантаження для розподілу вхідних запитів. Такий підхід дозволяє одночасно обробляти більше запитів і забезпечує відмовостійкість.

Розглянемо приклад налаштування горизонтального масштабування MCP.

### [.NET](../../../../05-AdvancedTopics/mcp-scaling)

```csharp
// ASP.NET Core MCP load balancing configuration
public class McpLoadBalancedStartup
{
    public void ConfigureServices(IServiceCollection services)
    {
        // Configure distributed cache for session state
        services.AddStackExchangeRedisCache(options =>
        {
            options.Configuration = Configuration.GetConnectionString("RedisConnection");
            options.InstanceName = "MCP_";
        });
        
        // Configure MCP with distributed caching
        services.AddMcpServer(options =>
        {
            options.ServerName = "Scalable MCP Server";
            options.ServerVersion = "1.0.0";
            options.EnableDistributedCaching = true;
            options.CacheExpirationMinutes = 60;
        });
        
        // Register tools
        services.AddMcpTool<HighPerformanceTool>();
    }
}
```

У наведеному коді ми:

- Налаштували розподілений кеш за допомогою Redis для зберігання стану сесії та даних інструментів.
- Увімкнули розподілене кешування в конфігурації сервера MCP.
- Зареєстрували високопродуктивний інструмент, який можна використовувати на кількох екземплярах MCP.

---

## Вертикальне масштабування та оптимізація ресурсів

Вертикальне масштабування зосереджене на оптимізації одного екземпляра сервера MCP для ефективної обробки більшої кількості запитів. Це можна досягти шляхом тонкого налаштування конфігурацій, використання ефективних алгоритмів і ефективного управління ресурсами. Наприклад, можна налаштувати пул потоків, таймаути запитів і обмеження пам’яті для покращення продуктивності.

Розглянемо приклад оптимізації сервера MCP для вертикального масштабування та управління ресурсами.

# [Java](../../../../05-AdvancedTopics/mcp-scaling)

```java
// Java MCP server with resource optimization
public class OptimizedMcpServer {
    public static McpServer createOptimizedServer() {
        // Configure thread pool for optimal performance
        int processors = Runtime.getRuntime().availableProcessors();
        int optimalThreads = processors * 2; // Common heuristic for I/O-bound tasks
        
        ExecutorService executorService = new ThreadPoolExecutor(
            processors,       // Core pool size
            optimalThreads,   // Maximum pool size 
            60L,              // Keep-alive time
            TimeUnit.SECONDS,
            new ArrayBlockingQueue<>(1000), // Request queue size
            new ThreadPoolExecutor.CallerRunsPolicy() // Backpressure strategy
        );
        
        // Configure and build MCP server with resource constraints
        return new McpServer.Builder()
            .setName("High-Performance MCP Server")
            .setVersion("1.0.0")
            .setPort(5000)
            .setExecutor(executorService)
            .setMaxRequestSize(1024 * 1024) // 1MB
            .setMaxConcurrentRequests(100)
            .setRequestTimeoutMs(5000) // 5 seconds
            .build();
    }
}
```

У наведеному коді ми:

- Налаштували пул потоків з оптимальною кількістю потоків залежно від кількості доступних процесорів.
- Встановили обмеження ресурсів, такі як максимальний розмір запиту, максимальна кількість одночасних запитів і таймаут запиту.
- Використали стратегію backpressure для плавного оброблення ситуацій перевантаження.

---

## Розподілена архітектура

Розподілені архітектури передбачають роботу кількох вузлів MCP разом для обробки запитів, спільного використання ресурсів і забезпечення резервування. Такий підхід підвищує масштабованість і відмовостійкість, дозволяючи вузлам взаємодіяти та координуватися через розподілену систему.

Розглянемо приклад реалізації розподіленої архітектури сервера MCP з використанням Redis для координації.

# [Python](../../../../05-AdvancedTopics/mcp-scaling)

```python
# Python MCP server in distributed architecture
from mcp_server import AsyncMcpServer
import asyncio
import aioredis
import uuid

class DistributedMcpServer:
    def __init__(self, node_id=None):
        self.node_id = node_id or str(uuid.uuid4())
        self.redis = None
        self.server = None
    
    async def initialize(self):
        # Connect to Redis for coordination
        self.redis = await aioredis.create_redis_pool("redis://redis-master:6379")
        
        # Register this node with the cluster
        await self.redis.sadd("mcp:nodes", self.node_id)
        await self.redis.hset(f"mcp:node:{self.node_id}", "status", "starting")
        
        # Create the MCP server
        self.server = AsyncMcpServer(
            name=f"MCP Node {self.node_id[:8]}",
            version="1.0.0",
            port=5000,
            max_concurrent_requests=50
        )
        
        # Register tools - each node might specialize in certain tools
        self.register_tools()
        
        # Start heartbeat mechanism
        asyncio.create_task(self._heartbeat())
        
        # Start server
        await self.server.start()
        
        # Update node status
        await self.redis.hset(f"mcp:node:{self.node_id}", "status", "running")
        print(f"MCP Node {self.node_id[:8]} running on port 5000")
    
    def register_tools(self):
        # Register common tools across all nodes
        self.server.register_tool(CommonTool1())
        self.server.register_tool(CommonTool2())
        
        # Register specialized tools for this node (could be based on node_id or config)
        if int(self.node_id[-1], 16) % 3 == 0:  # Simple way to distribute specialized tools
            self.server.register_tool(SpecializedTool1())
        elif int(self.node_id[-1], 16) % 3 == 1:
            self.server.register_tool(SpecializedTool2())
        else:
            self.server.register_tool(SpecializedTool3())
    
    async def _heartbeat(self):
        """Periodic heartbeat to indicate node health"""
        while True:
            try:
                await self.redis.hset(
                    f"mcp:node:{self.node_id}", 
                    mapping={
                        "lastHeartbeat": int(time.time()),
                        "load": len(self.server.active_requests),
                        "maxLoad": self.server.max_concurrent_requests
                    }
                )
                await asyncio.sleep(5)  # Heartbeat every 5 seconds
            except Exception as e:
                print(f"Heartbeat error: {e}")
                await asyncio.sleep(1)
    
    async def shutdown(self):
        await self.redis.hset(f"mcp:node:{self.node_id}", "status", "stopping")
        await self.server.stop()
        await self.redis.srem("mcp:nodes", self.node_id)
        await self.redis.delete(f"mcp:node:{self.node_id}")
        self.redis.close()
        await self.redis.wait_closed()
```

У наведеному коді ми:

- Створили розподілений сервер MCP, який реєструється в екземплярі Redis для координації.
- Реалізували механізм heartbeat для оновлення статусу вузла та навантаження в Redis.
- Зареєстрували інструменти, які можна спеціалізувати залежно від ID вузла, що дозволяє розподіляти навантаження між вузлами.
- Забезпечили метод завершення роботи для очищення ресурсів і зняття реєстрації вузла з кластера.
- Використали асинхронне програмування для ефективної обробки запитів і підтримки відгуку.
- Застосували Redis для координації та управління станом між розподіленими вузлами.

---

## Що далі

- [5.8 Security](../mcp-security/README.md)

**Відмова від відповідальності**:  
Цей документ було перекладено за допомогою сервісу автоматичного перекладу [Co-op Translator](https://github.com/Azure/co-op-translator). Хоча ми прагнемо до точності, будь ласка, майте на увазі, що автоматичні переклади можуть містити помилки або неточності. Оригінальний документ рідною мовою слід вважати авторитетним джерелом. Для критично важливої інформації рекомендується звертатися до професійного людського перекладу. Ми не несемо відповідальності за будь-які непорозуміння або неправильні тлумачення, що виникли внаслідок використання цього перекладу.